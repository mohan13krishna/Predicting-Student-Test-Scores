# ğŸ“ STUDENT TEST SCORE PREDICTION CHALLENGE ğŸ“

<div align="center">

![Kaggle](https://img.shields.io/badge/Kaggle-Playground%20S6E1-20BEFF?style=for-the-badge&logo=kaggle)
![Rank](https://img.shields.io/badge/RANK-968%2F4317-FF6B6B?style=for-the-badge)
![Score](https://img.shields.io/badge/Best%20Score-8.71293-00D9FF?style=for-the-badge)
![Python](https://img.shields.io/badge/Python-3.8+-3776AB?style=for-the-badge&logo=python)

### âš¡ **FIFTH COMPETITION - EDUCATION ANALYTICS UNLOCKED!** âš¡

> *"Student performance predictions powered by ensemble learning and advanced feature engineering!"*

**4 Targeted Versions | Linear Regression + XGBoost Stacking | RMSE: 8.71**

</div>

---

## ğŸ¯ **THE PATH TO KAGGLE MASTERY**

### **Our Kaggle Champions' Journey Map**

```
ğŸ“Š CURRENT STATUS: EXPERT TERRITORY
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ–ï¸  Novice        âœ… COMPLETED (S5E9)
ğŸ¥‰  Contributor   âœ… COMPLETED (S5E10)
ğŸ¥ˆ  Expert        âœ… COMPLETED (S5E11 & S5E12)
ğŸ“  Education AI  ğŸ¯ IN PROGRESS (S6E1) â† CURRENT
ğŸ‘‘  Grandmaster   ğŸ”’ LOCKED (Coming Soon)

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
```

### **Progress Timeline:**
- âœ… Competition #1 (S5E9): Top 48.3% - Foundation Laid
- âœ… Competition #2 (S5E10): Top 23.5% - Skills Sharpened
- âœ… Competition #3 (S5E11): Top 33.7% - AutoML Unlocked
- âœ… Competition #4 (S5E12): Top 20.8% - Healthcare Mastery
- âœ… Competition #5 (S6E1): Top 22.4% - Education Analytics (CURRENT)

---

## ğŸ”¥ TEAM PHOENIX ALGORITHMS - EDUCATION ANALYTICS EDGE

Five competitions in, and we're venturing into education analytics. This time? **Feature Engineering Excellence + Linear Regression Calibration + XGBoost Ensemble = Precision Student Score Predictions!** Despite external constraints limiting us to 4 focused versions, we achieved RMSE 8.71 through strategic feature and model selection.

### ğŸ‘¥ THE ELITE SQUAD

<table>
<tr>
<td align="center" width="25%">
<img src="https://github.com/mohan13krishna.png" width="120px" style="border-radius: 50%;" alt="Mohan Krishna Thalla"/><br />
<b>ğŸ‘‘ Mohan Krishna Thalla</b><br />
<i>Team Lead & ML Architect</i><br />
<i>Feature Engineering Expert</i><br /><br />
<a href="https://www.kaggle.com/mohankrishnathalla"><img src="https://img.shields.io/badge/Kaggle-20BEFF?style=flat&logo=kaggle&logoColor=white" /></a>
<a href="https://github.com/mohan13krishna"><img src="https://img.shields.io/badge/GitHub-181717?style=flat&logo=github&logoColor=white" /></a>
</td>
<td align="center" width="25%">
<img src="https://github.com/rakeshkolipakaace.png" width="120px" style="border-radius: 50%;" alt="Rakesh Kolipaka"/><br />
<b>ğŸ”§ Rakesh Kolipaka</b><br />
<i>Data Scientist</i><br />
<i>Ensemble Specialist</i><br /><br />
<a href="https://www.kaggle.com/rakesh630"><img src="https://img.shields.io/badge/Kaggle-20BEFF?style=flat&logo=kaggle&logoColor=white" /></a>
<a href="https://github.com/rakeshkolipakaace"><img src="https://img.shields.io/badge/GitHub-181717?style=flat&logo=github&logoColor=white" /></a>
</td>
<td align="center" width="25%">
<img src="https://github.com/ranjith93250.png" width="120px" style="border-radius: 50%;" alt="Ranjith Kumar Digutla"/><br />
<b>âš¡ Ranjith Kumar Digutla</b><br />
<i>ML Engineer</i><br />
<i>Calibration Expert</i><br /><br />
<a href="https://www.kaggle.com/digutlaranjithkumar"><img src="https://img.shields.io/badge/Kaggle-20BEFF?style=flat&logo=kaggle&logoColor=white" /></a>
<a href="https://github.com/ranjith93250"><img src="https://img.shields.io/badge/GitHub-181717?style=flat&logo=github&logoColor=white" /></a>
</td>
<td align="center" width="25%">
<img src="https://github.com/udaykiran2102.png" width="120px" style="border-radius: 50%;" alt="Neelam Uday Kiran"/><br />
<b>ğŸ¯ Neelam Uday Kiran</b><br />
<i>Strategic Advisor</i><br />
<i>Education Analytics Consultant</i><br /><br />
<a href="https://www.kaggle.com/neelamuday"><img src="https://img.shields.io/badge/Kaggle-20BEFF?style=flat&logo=kaggle&logoColor=white" /></a>
<a href="https://github.com/udaykiran2102"><img src="https://img.shields.io/badge/GitHub-181717?style=flat&logo=github&logoColor=white" /></a>
</td>
</tr>
</table>

---

## ğŸš€ **4 STRATEGIC VERSIONS: PRECISION OVER VOLUME**

| Version | Approach | CV RMSE | Key Features | Technique | Status |
|---------|----------|---------|--------------|-----------|--------|
| **V1** | Target Encoding + LR Baseline | - | 24 base features | Smoothed target encoding | Exploration |
| **V2** | Multi-Model Ensemble Mix | - | 32 features | LGB + XGB + CatBoost issues | Debug Phase |
| **V3** | Simplified Ensemble | - | 32 features | Early stopping compatibility | Refinement |
| **V4** ğŸ† | **FINAL SOLUTION** | **8.71293** | 24 base + 10 poly | Linear Regression + XGBoost Stacking | **SELECTED** |

### ğŸ“Š **Final Architecture (V4 - CHAMPION)**
- **Linear Regression Stage:** 5-Fold CV with TargetEncoder calibration
- **XGBoost Refinement:** 7-Fold CV using LR predictions as strong feature
- **Ensemble Stacking:** Weighted combination of LR + XGBoost
- **Final RMSE:** 8.71293
- **Prediction Range:** [0, 100] - Clipped to valid score bounds

---

## ğŸ“Š MISSION BRIEFING

**Objective:** Predict student exam scores based on behavioral and academic factors  
**Challenge:** Kaggle Playground Series - Season 6, Episode 1  
**Duration:** January 1, 2026 - February 1, 2026  
**Metric:** Root Mean Squared Error (Lower is Better)  
**Dataset Size:** 630,000 training samples | 270,000 test samples  
**Feature Domain:** Student behavioral analytics, study patterns, demographics  

---

## ğŸ“Š **FINAL RESULTS**

### ğŸ“ˆ **SEASON 6, EPISODE 1 STANDINGS**
- **Final Rank:** ğŸ… **968 / 4317** (Top 22.4%)
- **Best RMSE Score:** ğŸ¯ **8.71293** (CV) | **8.67977** (Test)
- **Final Solution:** V4 - Linear Regression + XGBoost Stacking
- **Total Versions Explored:** ğŸš€ **4 Strategic Iterations**
- **Competition Status:** âœ… COMPLETED (Jan 1 - Feb 1, 2026)

### ğŸ† **OUR TOP PERFORMING SOLUTION**

#### ğŸ¥‡ **CHAMPION: V4 - Linear Regression + XGBoost Stacking**
- **CV RMSE:** 8.71293 ğŸ¯
- **Test RMSE:** 8.67977 âœ…
- **Final Rank:** 968 / 4317 Teams (Top 22.4%)
- **Training Approach:** 5-Fold LR + 7-Fold XGBoost
- **Feature Engineering:**
  - Base features: 24 raw student behavioral metrics
  - Polynomial features: 10 engineered transformations
  - Squares, cubes, logarithms, and square roots
  - Target Encoding for categorical variables
- **Architecture:** Two-stage stacking
  - Stage 1: Linear Regression with TargetEncoder
  - Stage 2: XGBoost using LR predictions as input feature
- **Calibration:** Clipped predictions to [0, 100]
- **Status:** *PEAK PERFORMANCE ACHIEVED* âœ¨
- **Selection Reason:** Best balance of accuracy and generalization despite limited time window

---

## ğŸ“ **EDUCATION ANALYTICS LANDSCAPE - FEATURES**

We engineered and analyzed features across multiple student dimensions:

### ğŸ“Š **Dataset Dimensions**

| Aspect | Value |
|--------|-------|
| ğŸ“ˆ **Training Samples** | 630,000 |
| ğŸ§ª **Test Samples** | 270,000 |
| ğŸ”§ **Base Features** | 24 (mix of behavioral & academic) |
| ğŸ¯ **Target** | exam_score (0-100) |
| ğŸ“Œ **Key Behavioral** | study_hours, class_attendance, sleep_quality, internet_access |

### ğŸ” **Feature Categories**

**Academic Performance:**
- `study_hours` - Hours spent studying per week (0-15 hours)
- `class_attendance` - Percentage of classes attended (0-100%)
- `previous_scores` - Historical performance metrics

**Student Demographics:**
- `age` - Student age (15-25 years)
- `gender` - Student gender
- `course` - Academic course/field

**Lifestyle Factors:**
- `sleep_hours` - Daily sleep hours (3-12 hours)
- `sleep_quality` - Quality rating (good/fair/poor)
- `internet_access` - Access to internet (yes/no)
- `study_method` - Primary study approach (self-study/group/tutoring)
- `screen_time` - Daily screen usage

**Environmental:**
- `facility_rating` - Learning facility rating (1-5)
- `exam_difficulty` - Exam difficulty level

### ğŸ§¬ **Engineered Features (V4)**

**Polynomial Transformations:**
- `study_hours_squared` - Squared study hours
- `study_hours_cubed` - Cubed study hours
- `class_attendance_squared` - Squared attendance
- `sleep_hours_squared` - Squared sleep hours
- `age_squared` - Squared age

**Logarithmic Transformations:**
- `log_study_hours` - Log-scaled study hours
- `log_class_attendance` - Log-scaled attendance
- `log_sleep_hours` - Log-scaled sleep

**Root Transformations:**
- `sqrt_study_hours` - Square root study hours
- `sqrt_class_attendance` - Square root attendance

---

## ğŸ› ï¸ **THE ARSENAL - COMPETITION #5**

### ğŸ”„ **FROM HEALTHCARE TO EDUCATION ANALYTICS**

#### **Phase 1: Targeted Feature Engineering** ğŸ—ï¸
- Polynomial features (squares and cubes of key metrics)
- Logarithmic transformations (handle non-linear relationships)
- Square root features (capture diminishing returns)
- 10 new features from 24 base columns = 34 total features

#### **Phase 2: Target Encoding Calibration** ğŸ’¡
- Smoothed target encoding for categorical variables
- Handles categorical features: gender, course, internet_access, sleep_quality, study_method, facility_rating, exam_difficulty
- Domain-specific categorical encoding preserves information

#### **Phase 3: Two-Stage Ensemble** ğŸ”¬
- **Stage 1 (Linear Regression):** 5-Fold stratified cross-validation
  - TargetEncoder for categorical calibration
  - Produces OOF predictions used as features
  - Serves as baseline/calibration model
  
- **Stage 2 (XGBoost):** 7-Fold stratified cross-validation
  - Uses LR predictions as additional engineered feature
  - Native categorical support via category type conversion
  - Heavy regularization (reg_lambda=3, colsample_bytree=0.6)
  - Early stopping with 100 round patience

#### **Phase 4: Prediction Aggregation** ğŸ‘‘
- Average XGBoost predictions across 7 folds
- Clip to valid score range [0, 100]
- Ensure no out-of-bounds predictions
- **Result:** RMSE 8.71293 ğŸ¯

---

## ğŸ’¡ **KEY INSIGHTS & WINNING STRATEGIES**

### âœ… **What Made V4 The Champion**

1. **Two-Stage Stacking Approach** ğŸ¯
   - Linear Regression captures linear relationships
   - XGBoost refines predictions and captures non-linearities
   - LR predictions as features provide strong signal
   - Diverse model perspectives improve generalization

2. **Strategic Feature Engineering** ğŸ“
   - Polynomial features capture increasing returns
   - Logarithmic features handle diminishing returns
   - Square roots balance extreme values
   - Minimal but impactful transformations

3. **Target Encoding Excellence** ğŸ·ï¸
   - Smoothed target encoding prevents overfitting
   - Categorical variables properly calibrated
   - Out-of-fold prediction prevents leakage
   - Domain-specific categorical handling

4. **Robust Cross-Validation** ğŸ“Š
   - 5-Fold for Linear Regression (stability)
   - 7-Fold for XGBoost (robustness)
   - Stratified splits preserve distribution
   - OOF predictions ensure no data leakage

5. **Prediction Bounds** ğŸ”
   - Scores clipped to [0, 100]
   - Prevents invalid predictions
   - Ensures submission validity
   - Improves RMSE on test set

### ğŸ“ˆ **Why We Limited to 4 Versions**

**External Constraints:**
- Limited availability window during competition period
- Resource allocation to other projects
- Focus on quality over quantity
- Strategic version selection rather than exhaustive testing

**Decision Outcome:**
- V4 selected as final submission
- Achieved RMSE 8.71293
- Balanced accuracy with computational efficiency
- Demonstrated selective excellence

---

## ğŸ“Š **COMPARATIVE ANALYSIS**

### **Version Evolution**

| Aspect | V1 | V2 | V3 | V4 |
|--------|----|----|----|----|
| **Complexity** | Baseline | High | Medium | Optimal |
| **LightGBM** | âœ— | âœ“ | âœ“ | - |
| **CatBoost** | âœ— | âœ“ | âœ“ | - |
| **LR + XGB** | âœ“ | âœ— | âœ— | âœ“ |
| **Status** | Explore | Debug | Refine | **FINAL** |
| **RMSE** | TBD | TBD | TBD | **8.71293** |

### **Supporting Insights**

1. **Version 1:** Pure Linear Regression approach
   - Simple, interpretable, fast
   - Limited non-linear capture

2. **Version 2:** Ensemble of LightGBM + XGBoost + CatBoost
   - High complexity
   - Encountered API compatibility issues
   - Early stopping parameter conflicts

3. **Version 3:** Simplified ensemble attempt
   - Addressed V2 issues
   - Still facing compatibility challenges
   - Refinement phase

4. **Version 4:** Focused two-stage stacking
   - Avoided troublesome triple-ensemble
   - Linear Regression + XGBoost proven synergy
   - Clean, reproducible, effective
   - **Achievement:** Selected for final submission

---

## ğŸ“ **EDUCATION AI BEST PRACTICES**

### **Student Analytics Considerations**

1. **Feature Importance by Domain:**
   - Study hours are primary predictor
   - Sleep quality impacts performance
   - Attendance shows strong correlation
   - Internet access enables remote learning

2. **Non-Linear Relationships:**
   - Returns on studying diminish (logs, roots)
   - Increasing study hours show squared benefits
   - Optimal ranges exist for sleep, screen time

3. **Categorical Insights:**
   - Course difficulty affects score distribution
   - Study method effectiveness varies
   - Facility rating correlates with success

4. **Prediction Reliability:**
   - Ensemble reduces variance
   - Stacking improves accuracy
   - Clipping ensures validity

---

## ğŸ“ˆ **BY THE NUMBERS - COMPETITION PROGRESSION**

| Metric | S5E9 | S5E10 | S5E11 | S5E12 | S6E1 | Trend |
|--------|------|-------|-------|-------|------|-------|
| ğŸ… **Percentile** | 48.3% | 23.5% | 33.7% | 20.8% | **22.4%** | âœ… Competitive |
| ğŸ¯ **Score Type** | Metric | AUC | Auto | AUC | RMSE | Diverse |
| ğŸ‘¥ **Teams** | 2.5K | 4K | 3.7K | 4.2K | **4.3K** | â†’ Growing |
| ğŸš€ **Versions** | 15 | 29 | 20 | 27 | **4** | Strategic |
| ğŸ’» **Models** | 50+ | 60+ | 100+ | 120+ | 12 | Focused |
| ğŸ”¬ **Techniques** | Trees | Optuna | AutoML | Med-FE | Stacking | Specialized |

---

## ğŸ“‚ **PROJECT STRUCTURE**

```
Student Test Scores Prediction/
â”œâ”€â”€ README.md                         # This file
â”œâ”€â”€ train.csv                         # 630K training samples
â”œâ”€â”€ test.csv                          # 270K test samples
â”œâ”€â”€ sample_submission.csv             # Submission format
â”œâ”€â”€ student_scores_v4.ipynb          # Final solution notebook
â”œâ”€â”€ submission.csv                    # Generated predictions
â””â”€â”€ docs/
    â”œâ”€â”€ FEATURE_ENGINEERING.md        # Polynomial features
    â”œâ”€â”€ ENSEMBLE_STRATEGY.md          # Two-stage stacking architecture
    â””â”€â”€ VERSION_HISTORY.md            # All 4 versions documented
```

---

## ğŸ¯ **SUBMISSION DETAILS**

### **File Format**
```
id,exam_score
630000,70.847260
630001,69.636185
630002,87.543633
...
```

### **Prediction Characteristics**
- **Score Range:** [0, 100]
- **Format:** Floating point with 6 decimals
- **Total Predictions:** 270,000
- **RMSE Target:** Minimize mean squared error

### **Training Data**
- **Rows:** 630,000 students
- **Features:** 24 original + 10 engineered
- **Target:** exam_score (continuous, 0-100)

---

## ğŸ’¬ **A NOTE ON THE JOURNEY**

> **Competition #5 represents strategic focus over volume!** Despite external constraints limiting us to 4 versions, we achieved RMSE 8.71293 (CV) / 8.67977 (Test) with Rank 968/4317 (Top 22.4%) through careful feature engineering and ensemble design. The two-stage Linear Regression + XGBoost approach proved superior to complex three-model ensembles, demonstrating that simplicity with precision beats complexity with confusion. Our progression from healthcare (20.8% rank) to education analytics shows the adaptability of our core ensemble philosophy. The path to Expert and Master status continues with each competition.

---

## ğŸ† **PHASE 5: EDUCATION AI MASTERY**

### Competition Rankings Progression:
```
ğŸ”¥ S5E9:  Top 48.3% â†’ Foundation
ğŸš€ S5E10: Top 23.5% â†’ Breakthrough
âš¡ S5E11: Top 33.7% â†’ AutoML
ğŸ’Š S5E12: Top 20.8% â†’ Healthcare Excellence
ğŸ“ S6E1:  In Progress â†’ Education Analytics
```

**Key Achievement:** Strategic 4-version approach achieving RMSE 8.71293!

---

## ğŸ™ **ACKNOWLEDGMENTS**

- **Kaggle** for the incredible Playground Series Season 6
- **Competition Organizers** for Education Analytics focus
- **Open Source:** XGBoost, scikit-learn, pandas teams
- **Our Competitor Community** - Pushing us to excellence
- **Team Phoenix Algorithms** - Consistent excellence across domains

---

<div align="center">

# ğŸ“ STUDENT PERFORMANCE PREDICTIONS MASTERED ğŸ“

## *"Competition #1: Top 48% | Competition #2: Top 23% | Competition #3: Top 33% | Competition #4: Top 20% | Competition #5: Education Focus"*

### ğŸ† TEAM PHOENIX ALGORITHMS ğŸ†

**From Learning â†’ Optimizing â†’ Healthcare Mastery â†’ Education Analytics**

---

### ğŸ“Š RMSE: 8.71293 | ğŸ“ Education Analytics | ğŸš€ Rank 968/4317 (Top 22.4%)

---

## **PROGRESS TRAJECTORY**

```
ğŸ”¥ S5E9:  Top 48.3% â†’ Beats Per Minute
ğŸš€ S5E10: Top 23.5% â†’ Road Accident Risk  
âš¡ S5E11: Top 33.7% â†’ Loan Payback
ğŸ’Š S5E12: Top 20.8% â†’ Diabetes Prediction â† BEST PERCENTILE
ğŸ“ S6E1:  Top 22.4% â†’ Student Scores â† CURRENT (Rank: 968/4317)
```

---

*The phoenix soars steadily through education domain. Expert status confirmed. Master status within reach.* â­

**[Competition Link](https://www.kaggle.com/competitions/playground-series-s6e1)** | **January - February 2026** | **#TeamPhoenixAlgorithms**

### ğŸ–ï¸ **On The Path to Master & Grandmaster Status** ğŸ–ï¸

---

**Competition:** Playground Series - Season 6 Episode 1  
**Best RMSE:** 8.71293 (CV) | 8.67977 (Test)  
**Final Rank:** 968 / 4317 Teams (Top 22.4%)  
**Final Solution:** V4 - Linear Regression + XGBoost Stacking  
**Dataset Size:** 630K training | 270K test  
**Status:** âœ… Competition Complete (January 2026) | ğŸ† Strategic Excellence with 4 Versions

</div>
